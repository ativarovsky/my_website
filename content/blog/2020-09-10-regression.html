---
title: Modeling Global Life Expectancy vs Education  using  Least Squares Regression
author: "Alice Tivarovsky"
date: '2020-09-10'
slug: regression-life-expectancy
toc: true
tags:
  - Modeling
  - Biostatistics
  - Epidemiology
categories:
  - Modeling
  - Biostatistics
  - Epidemiology
---


<div id="TOC">

</div>

<div id="motivation" class="section level2">
<h2>Motivation</h2>
<p>In the context of ordinary least squares regression (OLS), the word “ordinary” implies that it’s mundane and uninteresting. But honestly, when I first learned the mechanics of OLS in an intro stats class, I found it incredibly insightful. It fascinated me that there is so much data being collected and analyzed by super powerful computers and being passed into fancy machine learning models that can literally predict the future. And yet, when you really dig down and get to the fundamentals, the true beginning, all it really comes from is distances between points.</p>
<p>In this post, we’ll explore the mechanics of ordinary least squares regression using global data on life expectancy collected by the World Health Organization. We’ll get down to some bare-bones concepts of regression modeling, analyze model diagnostics, compare two models, and attempt to validate the assumptions for performing linear regression in the first place.</p>
</div>
<div id="data-preparation" class="section level2">
<h2>Data Preparation</h2>
<div id="data-source" class="section level3">
<h3>Data Source</h3>
<p>We’ll be using the WHO’s life expectancy dataset, found on Kaggle <a href="https://www.kaggle.com/kumarajarshi/life-expectancy-who">here</a>.</p>
</div>
<div id="libraries" class="section level3">
<h3>Libraries</h3>
<pre class="r"><code>library(tidyverse)
library(skimr)
library(broom)

theme_set(theme_bw())</code></pre>
</div>
<div id="data-import-and-tidy" class="section level3">
<h3>Data Import and Tidy</h3>
<p>Reading in and having a quick look at the dataset:</p>
<pre class="r"><code>lifexp_df &lt;- 
  read.csv(file = &quot;./data/who_life_expectancy.csv&quot;) %&gt;% 
  janitor::clean_names() 

glimpse(lifexp_df)</code></pre>
<pre><code>## Rows: 2,938
## Columns: 22
## $ country                         &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghani…
## $ year                            &lt;int&gt; 2015, 2014, 2013, 2012, 2011, 2010, 20…
## $ status                          &lt;chr&gt; &quot;Developing&quot;, &quot;Developing&quot;, &quot;Developin…
## $ life_expectancy                 &lt;dbl&gt; 65.0, 59.9, 59.9, 59.5, 59.2, 58.8, 58…
## $ adult_mortality                 &lt;int&gt; 263, 271, 268, 272, 275, 279, 281, 287…
## $ infant_deaths                   &lt;int&gt; 62, 64, 66, 69, 71, 74, 77, 80, 82, 84…
## $ alcohol                         &lt;dbl&gt; 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.…
## $ percentage_expenditure          &lt;dbl&gt; 71.279624, 73.523582, 73.219243, 78.18…
## $ hepatitis_b                     &lt;int&gt; 65, 62, 64, 67, 68, 66, 63, 64, 63, 64…
## $ measles                         &lt;int&gt; 1154, 492, 430, 2787, 3013, 1989, 2861…
## $ bmi                             &lt;dbl&gt; 19.1, 18.6, 18.1, 17.6, 17.2, 16.7, 16…
## $ under_five_deaths               &lt;int&gt; 83, 86, 89, 93, 97, 102, 106, 110, 113…
## $ polio                           &lt;int&gt; 6, 58, 62, 67, 68, 66, 63, 64, 63, 58,…
## $ total_expenditure               &lt;dbl&gt; 8.16, 8.18, 8.13, 8.52, 7.87, 9.20, 9.…
## $ diphtheria                      &lt;int&gt; 65, 62, 64, 67, 68, 66, 63, 64, 63, 58…
## $ hiv_aids                        &lt;dbl&gt; 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1…
## $ gdp                             &lt;dbl&gt; 584.25921, 612.69651, 631.74498, 669.9…
## $ population                      &lt;dbl&gt; 33736494, 327582, 31731688, 3696958, 2…
## $ thinness_1_19_years             &lt;dbl&gt; 17.2, 17.5, 17.7, 17.9, 18.2, 18.4, 18…
## $ thinness_5_9_years              &lt;dbl&gt; 17.3, 17.5, 17.7, 18.0, 18.2, 18.4, 18…
## $ income_composition_of_resources &lt;dbl&gt; 0.479, 0.476, 0.470, 0.463, 0.454, 0.4…
## $ schooling                       &lt;dbl&gt; 10.1, 10.0, 9.9, 9.8, 9.5, 9.2, 8.9, 8…</code></pre>
<pre class="r"><code>skim(lifexp_df)</code></pre>
<table>
<caption><span id="tab:import">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">lifexp_df</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">2938</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">22</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">20</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<colgroup>
<col width="19%" />
<col width="13%" />
<col width="19%" />
<col width="5%" />
<col width="5%" />
<col width="8%" />
<col width="12%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">country</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">52</td>
<td align="right">0</td>
<td align="right">193</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">status</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">9</td>
<td align="right">10</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table style="width:100%;">
<colgroup>
<col width="23%" />
<col width="7%" />
<col width="10%" />
<col width="8%" />
<col width="8%" />
<col width="5%" />
<col width="7%" />
<col width="7%" />
<col width="7%" />
<col width="9%" />
<col width="4%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">year</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">2007.52</td>
<td align="right">4.61</td>
<td align="right">2000.00</td>
<td align="right">2004.00</td>
<td align="right">2008.00</td>
<td align="right">2012.00</td>
<td align="right">2.015000e+03</td>
<td align="left">▇▆▆▆▆</td>
</tr>
<tr class="even">
<td align="left">life_expectancy</td>
<td align="right">10</td>
<td align="right">1.00</td>
<td align="right">69.22</td>
<td align="right">9.52</td>
<td align="right">36.30</td>
<td align="right">63.10</td>
<td align="right">72.10</td>
<td align="right">75.70</td>
<td align="right">8.900000e+01</td>
<td align="left">▁▂▃▇▂</td>
</tr>
<tr class="odd">
<td align="left">adult_mortality</td>
<td align="right">10</td>
<td align="right">1.00</td>
<td align="right">164.80</td>
<td align="right">124.29</td>
<td align="right">1.00</td>
<td align="right">74.00</td>
<td align="right">144.00</td>
<td align="right">228.00</td>
<td align="right">7.230000e+02</td>
<td align="left">▇▆▂▁▁</td>
</tr>
<tr class="even">
<td align="left">infant_deaths</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">30.30</td>
<td align="right">117.93</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">3.00</td>
<td align="right">22.00</td>
<td align="right">1.800000e+03</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">alcohol</td>
<td align="right">194</td>
<td align="right">0.93</td>
<td align="right">4.60</td>
<td align="right">4.05</td>
<td align="right">0.01</td>
<td align="right">0.88</td>
<td align="right">3.76</td>
<td align="right">7.70</td>
<td align="right">1.787000e+01</td>
<td align="left">▇▃▃▂▁</td>
</tr>
<tr class="even">
<td align="left">percentage_expenditure</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">738.25</td>
<td align="right">1987.91</td>
<td align="right">0.00</td>
<td align="right">4.69</td>
<td align="right">64.91</td>
<td align="right">441.53</td>
<td align="right">1.947991e+04</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">hepatitis_b</td>
<td align="right">553</td>
<td align="right">0.81</td>
<td align="right">80.94</td>
<td align="right">25.07</td>
<td align="right">1.00</td>
<td align="right">77.00</td>
<td align="right">92.00</td>
<td align="right">97.00</td>
<td align="right">9.900000e+01</td>
<td align="left">▁▁▁▂▇</td>
</tr>
<tr class="even">
<td align="left">measles</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">2419.59</td>
<td align="right">11467.27</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">17.00</td>
<td align="right">360.25</td>
<td align="right">2.121830e+05</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">bmi</td>
<td align="right">34</td>
<td align="right">0.99</td>
<td align="right">38.32</td>
<td align="right">20.04</td>
<td align="right">1.00</td>
<td align="right">19.30</td>
<td align="right">43.50</td>
<td align="right">56.20</td>
<td align="right">8.730000e+01</td>
<td align="left">▅▅▅▇▁</td>
</tr>
<tr class="even">
<td align="left">under_five_deaths</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">42.04</td>
<td align="right">160.45</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">4.00</td>
<td align="right">28.00</td>
<td align="right">2.500000e+03</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">polio</td>
<td align="right">19</td>
<td align="right">0.99</td>
<td align="right">82.55</td>
<td align="right">23.43</td>
<td align="right">3.00</td>
<td align="right">78.00</td>
<td align="right">93.00</td>
<td align="right">97.00</td>
<td align="right">9.900000e+01</td>
<td align="left">▁▁▁▂▇</td>
</tr>
<tr class="even">
<td align="left">total_expenditure</td>
<td align="right">226</td>
<td align="right">0.92</td>
<td align="right">5.94</td>
<td align="right">2.50</td>
<td align="right">0.37</td>
<td align="right">4.26</td>
<td align="right">5.76</td>
<td align="right">7.49</td>
<td align="right">1.760000e+01</td>
<td align="left">▃▇▃▁▁</td>
</tr>
<tr class="odd">
<td align="left">diphtheria</td>
<td align="right">19</td>
<td align="right">0.99</td>
<td align="right">82.32</td>
<td align="right">23.72</td>
<td align="right">2.00</td>
<td align="right">78.00</td>
<td align="right">93.00</td>
<td align="right">97.00</td>
<td align="right">9.900000e+01</td>
<td align="left">▁▁▁▂▇</td>
</tr>
<tr class="even">
<td align="left">hiv_aids</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">1.74</td>
<td align="right">5.08</td>
<td align="right">0.10</td>
<td align="right">0.10</td>
<td align="right">0.10</td>
<td align="right">0.80</td>
<td align="right">5.060000e+01</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">gdp</td>
<td align="right">448</td>
<td align="right">0.85</td>
<td align="right">7483.16</td>
<td align="right">14270.17</td>
<td align="right">1.68</td>
<td align="right">463.94</td>
<td align="right">1766.95</td>
<td align="right">5910.81</td>
<td align="right">1.191727e+05</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">population</td>
<td align="right">652</td>
<td align="right">0.78</td>
<td align="right">12753375.12</td>
<td align="right">61012096.51</td>
<td align="right">34.00</td>
<td align="right">195793.25</td>
<td align="right">1386542.00</td>
<td align="right">7420359.00</td>
<td align="right">1.293859e+09</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">thinness_1_19_years</td>
<td align="right">34</td>
<td align="right">0.99</td>
<td align="right">4.84</td>
<td align="right">4.42</td>
<td align="right">0.10</td>
<td align="right">1.60</td>
<td align="right">3.30</td>
<td align="right">7.20</td>
<td align="right">2.770000e+01</td>
<td align="left">▇▃▁▁▁</td>
</tr>
<tr class="even">
<td align="left">thinness_5_9_years</td>
<td align="right">34</td>
<td align="right">0.99</td>
<td align="right">4.87</td>
<td align="right">4.51</td>
<td align="right">0.10</td>
<td align="right">1.50</td>
<td align="right">3.30</td>
<td align="right">7.20</td>
<td align="right">2.860000e+01</td>
<td align="left">▇▃▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">income_composition_of_resources</td>
<td align="right">167</td>
<td align="right">0.94</td>
<td align="right">0.63</td>
<td align="right">0.21</td>
<td align="right">0.00</td>
<td align="right">0.49</td>
<td align="right">0.68</td>
<td align="right">0.78</td>
<td align="right">9.500000e-01</td>
<td align="left">▁▁▅▇▆</td>
</tr>
<tr class="even">
<td align="left">schooling</td>
<td align="right">163</td>
<td align="right">0.94</td>
<td align="right">11.99</td>
<td align="right">3.36</td>
<td align="right">0.00</td>
<td align="right">10.10</td>
<td align="right">12.30</td>
<td align="right">14.30</td>
<td align="right">2.070000e+01</td>
<td align="left">▁▂▇▇▁</td>
</tr>
</tbody>
</table>
<p><br>
The dataset contains 2938 rows, spanning 22 variables. Every row represents a country and year combination - a total of 193 unique countries for every year from 2000 to 2015. We’ll limit the analysis to one year since having the same country repeated as a new row constitutes a repeated measure, which requires more sophisticated analysis than what we’re doing here. I’m going to (arbitrarily) choose 2012.</p>
<p>We also notice from the <code>glimpse()</code> output that we have quite a few missing values, so we’ll need to drop them from our analysis.</p>
<p>Of the 22 variables available, we’re really only going to focus on the following variables:
* <code>life_expectancy</code>: average life expectancy at birth, measured in years
* <code>schooling</code>: national average of years of formal education
* <code>status</code>: binary variable coded “Developed” or “Developing” (this will allow for some interesting stratification later on)</p>
<p>We will ignore the other variables for this analysis. Thus, our final dataset is as follows:</p>
<pre class="r"><code>final_df &lt;-
  lifexp_df %&gt;% 
  filter(year == &quot;2012&quot;) %&gt;% 
  drop_na(life_expectancy, schooling, status)</code></pre>
</div>
</div>
<div id="analysis" class="section level2">
<h2>Analysis</h2>
<div id="step-1-fit-and-interpret-models" class="section level3">
<h3>Step 1: Fit and Interpret Model(s)</h3>
<div id="ordinary-least-squares-regression---single-variable" class="section level4">
<h4>Ordinary Least Squares Regression - Single Variable</h4>
<p>To visualize the math behind OLS, let’s take a look at a scatterplot of life expectancy vs schooling.</p>
<pre class="r"><code>final_df %&gt;% 
  ggplot(aes(x = schooling, y = life_expectancy)) + 
  geom_point() +
  labs(
    title = &quot;Figure 1: Life Expectancy vs Schooling&quot;
  )</code></pre>
<p><img src="/blog/2020-09-10-regression_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>We have a pretty linear relationship with potentially some heteroscedasticity, which we’ll talk about in the assumptions section below. We’re going to ask R to fit a line through these data points and then we’ll break down how R came up with this line.</p>
<pre class="r"><code>model_1 &lt;-
  lm(data = final_df, life_expectancy ~ schooling)

summary(model_1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = life_expectancy ~ schooling, data = final_df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.8459  -2.5066   0.3488   3.2111  12.4766 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  41.8211     1.7213   24.30   &lt;2e-16 ***
## schooling     2.2932     0.1319   17.38   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.003 on 171 degrees of freedom
## Multiple R-squared:  0.6386, Adjusted R-squared:  0.6365 
## F-statistic: 302.1 on 1 and 171 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The coefficients, also known as the “beta” terms, are our regression parameters: the intercept, \(b_0 \), estimated as 41.821 years, represents the average life expectancy in a theoretical nation where the average years of schooling was 0. This value is meaningless because there are no nations with this average education level and to interpret a regression line beyond the scope of the data that generated it is an epic no-no of cardinal sin magnitude. The slope, \(b_1 \), is estimated as 2.293, and represents the rate of change in life expectancy per additional year of schooling, give or take an error term, \(_i \). So on average, countries with one additional year of schooling add 2.293 years their average life expectancy.</p>
<p>Thus, from the “true” population model:
<span class="math display">\[ y_i = \beta_0 + \beta*x_i + \epsilon_i \]</span></p>
<p>we get our fitted model:
<span class="math display">\[ y = 41.821 + 2.293*x  \]</span></p>
<p>Note that the F-statistic has a very small p-value (F = 302.1 on 1 and 171 DF, p-value: &lt; 2.2e-16). This means that our model is statistically significant. But since we only have one predictor term, the statistical significance of the model is same as the significance of the predictor term,<code>schooling</code> in this model.</p>
<p>To visualize the line, we use <code>geom_smooth</code> with a method = “lm” argument:</p>
<pre class="r"><code>final_df %&gt;% 
  ggplot(aes(x = schooling, y = life_expectancy)) + 
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, se = FALSE) +
  labs(
    title = &quot;Figure 2: Fitted Model&quot;
  )</code></pre>
<p><img src="/blog/2020-09-10-regression_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>We want to look closely at the distances between the datapoints and the line. Unfortunately, <code>lm()</code> doesn’t give us a dataframe output we can work with. This is where the <code>broom</code> package comes in handy. Specifically, the <code>augment()</code> function within <code>broom</code> allows us to build a dataframe with all kinds of useful information:</p>
<pre class="r"><code>model_1_df = 
  broom::augment(model_1)
model_1_df</code></pre>
<pre><code>## # A tibble: 173 × 8
##    life_expectancy schooling .fitted .resid    .hat .sigma   .cooksd .std.resid
##              &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
##  1            59.5       9.8    64.3 -4.79  0.0117    5.00 0.00551      -0.964 
##  2            76.9      14.2    74.4  2.52  0.00729   5.01 0.000936      0.505 
##  3            75.1      14.4    74.8  0.257 0.00773   5.02 0.0000104     0.0516
##  4            56        10.3    65.4 -9.44  0.00987   4.96 0.0179       -1.90  
##  5            75.9      13.8    73.5  2.43  0.00658   5.01 0.000789      0.488 
##  6            75.9      17.2    81.3 -5.36  0.0197    5.00 0.0118       -1.08  
##  7            74.4      12.7    70.9  3.46  0.00578   5.01 0.00139       0.693 
##  8            82.3      20.1    87.9 -5.61  0.0436    5.00 0.0300       -1.15  
##  9            88        15.7    77.8 10.2   0.0119    4.96 0.0253        2.05  
## 10            71.9      11.8    68.9  3.02  0.00637   5.01 0.00118       0.605 
## # … with 163 more rows</code></pre>
<p>The critical column here is the <strong>residuals</strong> vector, <code>.resid</code>, i.e. the vertical distances between the observed points and the fitted line. These values form the basis of regression modeling. To formalize, the residual (also known as the error term) is given as:
<span class="math display">\[ \epsilon_i = Y_i - \hat{Y_i} \]</span></p>
<p>where the little hat on the Y indicates that it’s the model estimate, meaning the y-value of the point on the blue line.</p>
<p>If we were to look at just the data points, close one eye, and draw a line through them, we’d probably come with something close to the blue line R gave us. But while we would be using the complex machinery of our brain’s pattern-recognition capacity, the actual math behind the blue line is fairly straightforward. It all comes down to finding a line that optimizes these residuals. In fact, when we say “least squares”, we’re referring to minimizing the squares of these values. Let’s visualize them here using <code>geom_segment()</code>:</p>
<pre class="r"><code>model_1_df %&gt;% 
  ggplot(aes(x = schooling, y = life_expectancy)) + 
  geom_point() +
  geom_segment(aes(xend = schooling, yend = .fitted), color = &quot;red&quot;) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE) +
  labs(
    title = &quot;Figure 3: Model Residuals&quot;
  )</code></pre>
<p><img src="/blog/2020-09-10-regression_files/figure-html/unnamed-chunk-6-1.png" width="672" />
<br>
The red lines are the residuals and the way R computes the fitted model is by minimizing the squares of these red lines. The residuals are squared to avoid cancellation when adding positive and negative values.</p>
</div>
<div id="ols---multi-variable-modeling" class="section level4">
<h4>OLS - Multi-variable Modeling</h4>
<p>In the real world, we’re hardly ever working with just one predictor. The beauty of regression modeling lies in its flexibility - we can add as many predictors to the right side of the equation as we want, and they don’t need to be continuous variables. We can add categorical predictors using “dummy variables” (explained <a href="https://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH717-QuantCore/PH717_MultipleVariableRegression/PH717_MultipleVariableRegression4.html">here</a>). There is of course a trade-off in flexibility if you add tons of predictors, and generally speaking, you should only add predictors that make theoretical sense and keep your model parsimonious.</p>
<p>It should also be clear that when you add a second predictor, you’re no longer working in two-dimensional space. You need a third dimension to describe the relationship between the dependent and independent variables. You will also no longer be fitting a regression line, but a regression plane, which is very cool. We won’t make such a plane here, but you can find an example <a href="https://data-se.netlify.app/2018/12/13/visualizing-a-regression-plane-two-predictors/">here</a>.</p>
<p>Let’s take a look at our third variable, <code>status</code>, indicating whether the observation (nation) is considered developed or developing by the WHO’s definition. First, let’s do some visualization:</p>
<pre class="r"><code>final_df %&gt;% 
  mutate(index = row_number()) %&gt;% 
  ggplot(aes(x = index, y = life_expectancy, group = status)) + 
  geom_point(aes(color = status)) + 
  labs(
    title = &quot;Figure 4: Life Expectancy Difference by Status &quot;
  )</code></pre>
<p><img src="/blog/2020-09-10-regression_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Clearly, countries in the developed world have higher life expectancy. But the question we want to answer is whether the relationship between schooling and life expectancy <strong>changes</strong> between developed and developing countries. In other words, if we fit a line using just the green points, and another using just the red points, would the slopes of those lines be different? And if so, how different? Regression modeling gives us an easy way to find out - all we need to do is add <code>status</code> as a predictor term:</p>
<pre class="r"><code>model_2 = 
  lm(data = final_df, life_expectancy ~ schooling + status)
summary(model_2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = life_expectancy ~ schooling + status, data = final_df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.7545  -3.1520   0.5462   3.5773  12.4017 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       45.4937     2.7312  16.657   &lt;2e-16 ***
## schooling          2.1420     0.1578  13.578   &lt;2e-16 ***
## statusDeveloping  -2.1010     1.2176  -1.725   0.0863 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.975 on 170 degrees of freedom
## Multiple R-squared:  0.6448, Adjusted R-squared:  0.6406 
## F-statistic: 154.3 on 2 and 170 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Again, the p-value on our F-statistic is significant, meaning that there is a statistically significant relationship between the outcome and the combination of predictors.</p>
<p>Our new model statement is:</p>
<p><span class="math display">\[ life_expectancy = 45.4937 + 2.1420*schooling - 2.1010*status \]</span></p>
<p>where the coefficient of the <code>status</code> variable indicates that, on average, after we control for schooling, there is a difference in life expectancy of 2.1 years between developing and developed countries. However, note that the p-value for the status predictor is 0.08 (p &gt; 0.05), meaning that controlling for education, status is <strong>not</strong> a significant predictor of life expectancy. This does not mean that status <strong>alone</strong> is not a significant predictor of life expectancy. It just means that after we control for education by putting it in the model, the effect of status mostly washes away.</p>
<p>To illustrate:</p>
<pre class="r"><code>model_3 = lm(data = final_df, life_expectancy ~ status)
summary(model_3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = life_expectancy ~ status, data = final_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -19.408  -5.408   1.307   5.592  14.892 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)        80.393      1.330  60.456  &lt; 2e-16 ***
## statusDeveloping  -11.285      1.458  -7.742 8.17e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.161 on 171 degrees of freedom
## Multiple R-squared:  0.2596, Adjusted R-squared:  0.2552 
## F-statistic: 59.94 on 1 and 171 DF,  p-value: 8.169e-13</code></pre>
<p>Clearly, status is a highly significant factor in life expectancy when considered by itself. This is even clearer in a plot:</p>
<pre class="r"><code>final_df %&gt;% 
  ggplot(aes(x = status, y = life_expectancy)) + 
  geom_point() + 
  stat_smooth(method = &quot;lm&quot;, se = FALSE) +
  labs(
    title = &quot;Figure 5: Status vs Life Expectancy&quot;
  )</code></pre>
<p><img src="/blog/2020-09-10-regression_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>What the p-value for status in <code>model_2</code> <strong>does</strong> tell us is that the relationship between schooling and life expectancy does not change significantly between developed and developing countries. Again, to visualize:</p>
<pre class="r"><code>final_df %&gt;% 
  ggplot(aes(x = schooling, y = life_expectancy, group = status)) + 
  geom_point(aes(color = status))</code></pre>
<p><img src="/blog/2020-09-10-regression_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>The point is that even though we have two distinct clusters, the regression line is more or less the same between them.</p>
<p>For our purposes, we’re going to take all of this to mean that we should get rid of the <code>status</code> term in our model. This is not to say that all predictors with p-values &lt; 0.05 don’t belong in your model. We’re not going to go down that rabbit hole, but you can find people arguing the matter at length on <a href="https://stackexchange.com/">stackexchange</a>. This is just a decision I’m making based on the evidence.</p>
<p>So now that we’ve settled on <code>model_1</code>, how do we figure out how “good” it is? That’s where regression diagnostics come in.</p>
</div>
</div>
<div id="step-2-figure-out-how-good-your-model-is" class="section level3">
<h3>Step 2: Figure out how “good” your model is</h3>
<p>Smith et al (2017) describe regression diagnostics as an “art”, referring to the absence of a structured process for model evaluation. After looking through a digital bushel of papers, I would have to agree. If you spend any time looking at scientific work employing linear regression models, you’ll be hard-pressed to find any consistency in the measures scientists use to evaluate their models (if they use such measures at all).</p>
<p>However, it’s clear that simply fitting a linear model is not enough. While there are many different constructs and measures to evaluate model performance, I would like to, at minimum, understand the magnitude of a model’s error, its predictive capacity, the impact of outliers and influential observations, and whether it meets the assumptions for linear modeling in the first place. We’ll look at this last topic in the <a href = "#step-3-:-validate-regression-assumptions">next section</a>.</p>
<p>First, let’s look at perhaps the most common model diagnostic, \(R^2\), or, more formally, the coefficient of determination. For a single-variable model, \(R^2\), is just the square of our friend, the Pearson correlation coefficient. It turns out that this is actually a much more useful quantity:</p>
<p><span class="math display">\[ R^2 = \frac {\Sigma_{i=1}^n (y_i - \hat y_i)^2} {\Sigma_{i=1}^n (y_i - \bar y_i)^2} \]</span></p>
<p>\( R^2 \) is powerful because it’s completely intuitive - it equals the percentage of variance in the outcome explained by the predictor(s). This is part of the <code>summary()</code> output:</p>
<pre class="r"><code>summary(model_1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = life_expectancy ~ schooling, data = final_df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.8459  -2.5066   0.3488   3.2111  12.4766 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  41.8211     1.7213   24.30   &lt;2e-16 ***
## schooling     2.2932     0.1319   17.38   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.003 on 171 degrees of freedom
## Multiple R-squared:  0.6386, Adjusted R-squared:  0.6365 
## F-statistic: 302.1 on 1 and 171 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>For the single-variable model, \( R^2 = 0.6386 \), so 63.86% of the variance in a nation’s life expectancy can be explained by its linear relationship to education. For a single variable model, this is a pretty high \( R^2 \). Note that when we fit <code>model_2</code>, the \( R^2 \) only went up to 64.48%, all while taking away a degree of freedom from the model - further evidence that <code>status</code> is not a worthwhile predictor in this context.</p>
<p>To quantify the model’s error, which, again, comes down to residuals, we can look at Root Mean Square Error (<strong>RMSE</strong>), another highly common regression diagnostic. It is defined as:</p>
<p><span class="math display">\[ RMSE = \sqrt \frac {\Sigma_{i=1}^n (\hat y_i - y_i)^2}  {n} \]</span></p>
<p>and as you might intuit from the formula, RMSE is a measure of the standard deviation of residuals. RMSE can be computed using the <code>metrics</code> package, or just using a quick manual calculation:</p>
<pre class="r"><code>sqrt(mean(model_1_df$.resid^2))</code></pre>
<pre><code>## [1] 4.97418</code></pre>
<p>Our calculated RMSE of 4.974 indicates that actual life expectancy deviates from life expectancy predicted by the model by about 5 years, on average.</p>
<p>Another way to assess model performance is to figure out whether it was influenced by a small set of influential observations. Perhaps our model started out as a perfectly nice model, chugging along, predicting stuff with few mistakes. But then it came across some highly influential characters - data points that don’t hang with the pack, non-conformers, and they applied heavy influence and ultimately swayed our model off its course.</p>
<p>In truth, the study of outliers and influential observation is a whole thing and worthy of its own project. For now, let’s do two things. First, let’s look at Figure 2 and acknowledge that there’s not much visual evidence of outliers. Second, let’s do a quick check using <a href="https://www.mathworks.com/help/stats/cooks-distance.html#:~:text=Cook&#39;s%20distance%20is%20the%20scaled,on%20the%20fitted%20response%20values.">Cook’s distance</a>, <code>.cooksd</code> in our <code>augment()</code> dataframe.</p>
<p>Cook’s distance is a metric based on <a href="https://online.stat.psu.edu/stat501/lesson/11/11.4">deleted residuals</a> and is calculated for each data point in the set. It is a measure of the difference that would occur in our predicted values if we were to re-run the regression model without that point. Let’s look at the Cook’s distances in <code>model_1</code>:</p>
<pre class="r"><code>model_1_df %&gt;% 
  arrange(desc(.cooksd)) %&gt;% 
  inner_join(lifexp_df, on = life_expectancy) %&gt;% 
  head()</code></pre>
<pre><code>## # A tibble: 6 × 28
##   life_exp…¹ schoo…² .fitted .resid    .hat .sigma .cooksd .std.…³ country  year
##        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;
## 1       63.6     5      53.3  10.3  0.0473    4.95  0.111     2.11 Eritrea  2012
## 2       63       5.1    53.5   9.48 0.0462    4.96  0.0912    1.94 Niger    2012
## 3       49.7     9.1    62.7 -13.0  0.0149    4.92  0.0518   -2.62 Sierra…  2012
## 4       77       9.9    64.5  12.5  0.0113    4.92  0.0360    2.51 Bangla…  2012
## 5       52.2    11      67.0 -14.8  0.00785   4.89  0.0351   -2.98 Lesotho  2012
## 6       52.7     9.7    64.1 -11.4  0.0121    4.94  0.0321   -2.29 Congo    2001
## # … with 18 more variables: status &lt;chr&gt;, adult_mortality &lt;int&gt;,
## #   infant_deaths &lt;int&gt;, alcohol &lt;dbl&gt;, percentage_expenditure &lt;dbl&gt;,
## #   hepatitis_b &lt;int&gt;, measles &lt;int&gt;, bmi &lt;dbl&gt;, under_five_deaths &lt;int&gt;,
## #   polio &lt;int&gt;, total_expenditure &lt;dbl&gt;, diphtheria &lt;int&gt;, hiv_aids &lt;dbl&gt;,
## #   gdp &lt;dbl&gt;, population &lt;dbl&gt;, thinness_1_19_years &lt;dbl&gt;,
## #   thinness_5_9_years &lt;dbl&gt;, income_composition_of_resources &lt;dbl&gt;, and
## #   abbreviated variable names ¹​life_expectancy, ²​schooling, ³​.std.resid</code></pre>
<p>We can see that our top five most influential observations are Eritrea, Niger, Sierra Leone, Bangladesh, and Lesotho. To complement using a quick visualization:</p>
<pre class="r"><code>model_1_df %&gt;% 
  mutate(id = row_number()) %&gt;% 
  ggplot(aes(x = id, y = .cooksd)) +
  geom_point() + 
  theme_bw()</code></pre>
<p><img src="/blog/2020-09-10-regression_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>We see three points (Eritrea, Niger, Sierra Leone) that stand out from the pack. These observations are not just outliers, but <strong>influential outliers</strong>, meaning they swayed our model towards themselves, compromising its predictive capability.</p>
<p>So what do we do with them? You can find lots of different “rules of thumb” online, dictating which Cook’s distance might prompt removal of a data point from your set, but what makes most sense to me in this situation is to accept the data as they are. We don’t have any super high leverage points here, and even if we did, we would need justification for removing them. As you can probably intuit, few natural processes are simple enough to be accurately explained by a linear model. Hence the discovery/invention of more sophisticated probabilistic models and machine learning.</p>
</div>
<div id="step-3-validate-regression-assumptions" class="section level3">
<h3>Step 3: Validate Regression Assumptions</h3>
<p>Now that we’ve done all this work, we need to figure out whether any of it was worthwhile. This is a bit of a nuisance with linear regression - you can’t really check the assumptions before you fit the model because you need the model to know if the assumptions were satisfied.</p>
<p>You’ll often see regression assumptions summarized as follows:
1. <strong>Linearity</strong>: If the data aren’t linear, don’t fit a linear model. We looked at the scatterplot in step 1 and found it was pretty linear.
2. <strong>Independence</strong>: Observations shouldn’t be clustered or have any influence on other observations. It’s hard to say this is the case in our situation, since it’s pretty clear that developing countries and developed countries cluster together and through political and economic means influence their neighbors’ policies and practices.
3. <strong>Normality of residuals</strong>: If you look at the residuals in Figure 3 and superimpose little sideways bell curves along the regression line, you should see that the residuals are normally distributed. Basically, the majority of the residuals are located close to the line and a few are found further away. This is best explained visually:</p>
<p><img src="/image/residuals.png" style="width:50.0%" /></p>
<p>It looks like this is generally the case for our model, using the trusty eyeball method.
4. <strong>Homoscedasticity</strong> <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>: Homoscedasticity means that the variance of the residual terms is somewhat constant. If your data are heteroscedastic, linear regression modeling is probably not a good choice. Again, this is better visualized:</p>
<p><img src="/image/heteroscedasticity.png" style="width:50.0%" /></p>
<p>When we first looked at our data in scatterplot form <a href = "#ordinary-least-squares-regression-single-variable">above</a>, we noted some definite heteroscedasticity. Let’s look at it again:</p>
<pre class="r"><code>final_df %&gt;% 
  ggplot(aes(x = schooling, y = life_expectancy)) + 
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, se = FALSE) +
  labs(
    title = &quot;Life Expectancy vs Schooling&quot;
  )</code></pre>
<p><img src="/blog/2020-09-10-regression_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Below 10 years of schooling, our residuals are much wider than on the other side of 10. There might be some intuitive explanation for this observation. For instance, it’s possible that there’s a distinction between countries where a majority of the population finishes what is considered high school in the US and in countries where that isn’t the case, years of schooling doesn’t matter as much as other factors - like access to clean water and nutritious foods.</p>
<p>All in all, checking regression assumptions convinced me that linear regression is not really the best choice here. But on the bright side, we learned some good fundamental principles about how statistical modeling works.</p>
</div>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>Hopefully you learned a bit about ordinary least squares regression and how to evaluate simple regression models. After going through a fairly brief analysis, we learned that:</p>
<ul>
<li>Globally, the relationship between life expectancy and education based on WHO data is fairly linear, with a simple modeling resulting in an \(R^2 \&gt; of 0.64.</li>
<li>Few influential outliers were present in the data.</li>
<li>In order to build a more robust model, data should be limited to a range where the linear regression assumptions (namely, homoscedasticity) are met. Otherwise, a more robust modeling strategy should be used.</li>
</ul>
</div>
<div id="further-reading" class="section level2">
<h2>Further Reading</h2>
<ul>
<li>In-depth <a href="https://quantdev.ssri.psu.edu/sites/qdev/files/02_RegressionReview_Continuous%2C_Ordinal%2C_and_Binary_Outcomes__2018_0.html">overview</a> of regression modeling with different types of predictor variables</li>
<li>Good intro to <a href="https://uc-r.github.io/model_selection">model selection</a></li>
<li><a href="https://medium.com/@amanbamrah/how-to-evaluate-the-accuracy-of-regression-results-b38e5512afd3">Explanation</a> of differences between RMSE, \( R^2 \), and other measures of model error</li>
<li>A nice interactive OLS <a href="https://setosa.io/ev/ordinary-least-squares-regression/">explainer</a></li>
<li>Everything you ever wanted to know about residuals: <a href="https://drsimonj.svbtle.com/visualising-residuals" class="uri">https://drsimonj.svbtle.com/visualising-residuals</a></li>
<li>Great <a href="http://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/20/lecture-20.pdf">lecture</a> on outliers</li>
</ul>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<ul>
<li>Jenkins-Smith, H. C. (2017). Quantitative Research Methods for Political Science, Public Policy and Public Administration (With Applications in R): 3rd Edition. University of Oklahoma</li>
<li>Field, A. P., Miles, J., &amp; Field, Z. (2012). Discovering statistics using R. Thousand Oaks, CA.</li>
</ul>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>If you’re a spelling bee organizer, I recommend adding this word (mostly because I still misspell it almost every time I write it).<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
